<HEAD>
<TITLE><tt>genlex</tt>: a generic lexical analyzer </TITLE>
</HEAD>
<BODY>
<A HREF="node15.8.html"><IMG SRC="next_motif.gif"></A>
<A HREF="node15.6.html"><IMG SRC="previous_motif.gif"></A>
<A HREF="index.html"><IMG SRC="contents_motif.gif"></A>
<HR>
<H2><tt>genlex</tt>: a generic lexical analyzer </H2>
<P>
<A name="s:genlex"></A>

<dl><dd>
 This module implements a simple ``standard'' lexical analyzer, presented
   as a function from character streams to token streams. It implements
   roughly the lexical conventions of Caml, but is parameterized by the
   set of keywords of your language. 
</dl>
<pre>
type token =
    Kwd of string
  | Ident of string
  | Int of int
  | Float of float
  | String of string
  | Char of char
</pre>
<dl><dd>
 The type of tokens. The lexical classes are: <tt>Int</tt> and <tt>Float</tt>
           for integer and floating-point numbers; <tt>String</tt> for
           string literals, enclosed in double quotes; <tt>Char</tt> for
           character literals, enclosed in backquotes; <tt>Ident</tt> for
           identifiers (either sequences of letters, digits, underscores
           and quotes, or sequences of ``operator characters'' such as
           <tt>+</tt>, <tt>*</tt>, etc); and <tt>Kwd</tt> for keywords (either identifiers or
           single ``special characters'' such as <tt>(</tt>, <tt>}</tt>, etc). 
</dl>
<pre>
value make_lexer: string list -&gt; (char stream -&gt; token stream)
</pre>
<dl><dd>
 Construct the lexer function. The first argument is the list of
           keywords. An identifier <tt>s</tt> is returned as <tt>Kwd s</tt> if <tt>s</tt>
           belongs to this list, and as <tt>Ident s</tt> otherwise.
           A special character <tt>s</tt> is returned as <tt>Kwd s</tt> if <tt>s</tt>
           belongs to this list, and cause a lexical error (exception
           <tt>Parse_error</tt>) otherwise. Blanks and newlines are skipped.
           Comments delimited by <tt>(*</tt> and <tt>*)</tt> are skipped as well,
           and can be nested. 
</dl>
<dl><dd>
Example: a lexer suitable for a desk calculator is obtained by
<pre>
           let lexer = make_lexer ["+";"-";"*";"/";"let";"="; "("; ")"]
</pre><P>
           The associated parser would be a function from <tt>token stream</tt>
           to, for instance, <tt>int</tt>, and would have rules such as:
<pre>
           let parse_expr = function
                  [&lt; 'Int n &gt;] -&gt; n
                | [&lt; 'Kwd "("; parse_expr n; 'Kwd ")" &gt;] -&gt; n
                | [&lt; parse_expr n1; (parse_remainder n1) n2 &gt;] -&gt; n2
           and parse_remainder n1 = function
                  [&lt; 'Kwd "+"; parse_expr n2 &gt;] -&gt; n1+n2
                | ...
</pre><P>
</dl>
<HR>
<A HREF="node15.8.html"><IMG SRC="next_motif.gif"></A>
<A HREF="node15.6.html"><IMG SRC="previous_motif.gif"></A>
<A HREF="index.html"><IMG SRC="contents_motif.gif"></A>
